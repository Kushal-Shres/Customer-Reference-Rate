{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of hyperparameter optimization in machine learning is to find the hyperparameters of a given machine learning algorithm that return the best performance as measured on a validation set. (Hyperparameters, in contrast to model parameters, are set by the machine learning engineer before training. The number of trees in a random forest is a hyperparameter while the weights in a neural network are model parameters learned during training. if you are looking for better scores in this competition, you may ignore the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are four common methods of hyperparameter optimization for machine learning in order of increasing efficiency:\n",
    "\n",
    "* Manual\n",
    "* Grid search\n",
    "* Random search\n",
    "* Bayesian model-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel provides an example to tuning the hyper - parameters using Bayesian Optimization on Light GBM (one of the most popular algorithm in Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Lgbm\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Suppr warning\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "# Data processing, metrics and modeling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold,KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "# Lgbm\n",
    "import lightgbm as lgb\n",
    "# Suppr warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import itertools\n",
    "from scipy import interp\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "import os\n",
    "print(os.listdir('../input/'))\n",
    "Random_Seed = 4520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "DataFile = pd.read_csv('../input/creditcard.csv')\n",
    "train_df = DataFile.drop(['Time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(train_df)\n",
    "features.remove('Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut tr and val\n",
    "bayesian_tr_idx, bayesian_val_idx = train_test_split(train_df, test_size = 0.3, random_state = 42, stratify = train_df['Class'])\n",
    "bayesian_tr_idx = bayesian_tr_idx.index\n",
    "bayesian_val_idx = bayesian_val_idx.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Take the hyper parameters you want to consider\n",
    "\n",
    "paramsLGB = {\n",
    "    'learning_rate': (0.001,0.005),\n",
    "    'num_leaves': (50, 500), \n",
    "    'bagging_fraction' : (0.1, 0.9),\n",
    "    'feature_fraction' : (0.1, 0.9),\n",
    "    'min_child_weight': (0.00001, 0.01),   \n",
    "    'min_data_in_leaf': (20, 70),\n",
    "    'max_depth':(-1,50),\n",
    "    'reg_alpha': (1, 2), \n",
    "    'reg_lambda': (1, 2)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGB_bayesian(\n",
    "    learning_rate,\n",
    "    num_leaves, \n",
    "    bagging_fraction,\n",
    "    feature_fraction,\n",
    "    min_child_weight, \n",
    "    min_data_in_leaf,\n",
    "    max_depth,\n",
    "    reg_alpha,\n",
    "    reg_lambda\n",
    "     ):\n",
    "    \n",
    "    # LightGBM expects next three parameters need to be integer. \n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "    \n",
    "\n",
    "    param = {\n",
    "              'num_leaves': num_leaves, \n",
    "              'min_data_in_leaf': min_data_in_leaf,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'feature_fraction' : feature_fraction,\n",
    "              'learning_rate' : learning_rate,\n",
    "              'max_depth': max_depth,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'objective': 'binary',\n",
    "              'save_binary': True,\n",
    "              'seed': Random_Seed,\n",
    "              'feature_fraction_seed': Random_Seed,\n",
    "              'bagging_seed': Random_Seed,\n",
    "              'drop_seed': Random_Seed,\n",
    "              'data_random_seed': Random_Seed,\n",
    "              'boosting_type': 'gbdt',\n",
    "              'verbose': 1,\n",
    "              'is_unbalance': False,\n",
    "              'boost_from_average': True,\n",
    "              'metric':'auc'}    \n",
    "    \n",
    "    oof = np.zeros(len(train_df))\n",
    "    trn_data= lgb.Dataset(train_df.iloc[bayesian_tr_idx][features].values, label=train_df.iloc[bayesian_tr_idx]['Class'].values)\n",
    "    val_data= lgb.Dataset(train_df.iloc[bayesian_val_idx][features].values, label=train_df.iloc[bayesian_val_idx]['Class'].values)\n",
    "\n",
    "    clf = lgb.train(param, trn_data,  num_boost_round=50, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds = 50)\n",
    "    \n",
    "    oof[bayesian_val_idx]  = clf.predict(train_df.iloc[bayesian_val_idx][features].values, num_iteration=clf.best_iteration)  \n",
    "    \n",
    "    score = roc_auc_score(train_df.iloc[bayesian_val_idx]['Class'].values, oof[bayesian_val_idx])\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_BO = BayesianOptimization(LGB_bayesian, paramsLGB, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LGB_BO.space.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points = 9\n",
    "n_iter = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-' * 130)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_BO.max['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lgb = {\n",
    "        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), \n",
    "        'num_leaves': int(LGB_BO.max['params']['num_leaves']), \n",
    "        'learning_rate': LGB_BO.max['params']['learning_rate'],\n",
    "        'min_child_weight': LGB_BO.max['params']['min_child_weight'],\n",
    "        'bagging_fraction': LGB_BO.max['params']['bagging_fraction'], \n",
    "        'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n",
    "        'reg_lambda': LGB_BO.max['params']['reg_lambda'],\n",
    "        'reg_alpha': LGB_BO.max['params']['reg_alpha'],\n",
    "        'max_depth': int(LGB_BO.max['params']['max_depth']), \n",
    "        'objective': 'binary',\n",
    "        'save_binary': True,\n",
    "        'seed': Random_Seed,\n",
    "        'feature_fraction_seed': Random_Seed,\n",
    "        'bagging_seed': Random_Seed,\n",
    "        'drop_seed': Random_Seed,\n",
    "        'data_random_seed': Random_Seed,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'is_unbalance': False,\n",
    "        'boost_from_average': True,\n",
    "        'metric':'auc'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use these parameters for your final model. You can use a similar technique for any other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some references for the text  : https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\n",
    "\n",
    "Some references for the code : https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt (Please upvote his kernel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
